{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c76ea3",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c9d9b",
   "metadata": {},
   "source": [
    "##### 7th exercise: <font color=\"#C70039\">Work with Variational Autoencoders (Generative Model)</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Date:   07.11.2022\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/848/1*6uuK7GpIbfTb-0chqFwXXw.png\" style=\"float: center;\" width=\"400\">\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too.\n",
    "* This applies to all exercises throughout this course.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "A Variational Autoencoder (VAE), instead of compressing its input image into a fixed code in the latent space (as the classic autoencoder does), turns the input image into the parameters of a statistical distribution: a mean and a variance.\n",
    "\n",
    "This implies / imputes that the input image has been generated by a statistical process and that the randomness of this process should be taken into accounting during encoding and decoding. \n",
    "\n",
    "The VAE then uses the mean and variance parameters to randomly sample one element of that distribution, and decodes that element back to the original input. \n",
    "\n",
    "The stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere: every point sampled in the latent space is decoded to a valid output.\n",
    "\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time. \n",
    "    * try to understand each single step.\n",
    "4. the notebooks code, especially keras is sometimes utilized a bit cumbersome. Try to optimize the code where you feel necessary.\n",
    "5. experiment with different hyperparameters (search for the keyword 'task')\n",
    "6. describe the three different loss curve plots. What do they show? Is this what you expected?\n",
    "7. the main task is to visualize the latent space, the encoder has created. If you set high dimensions for the latent dim you can use T_SNE (plot 4).\n",
    "8. describe the latent space with respect to its structure. Is this what you expected from a VAE?\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26e3d9",
   "metadata": {},
   "source": [
    "### VAEs\n",
    "This code demonstrates a VAE using the MNIST dataset.\n",
    "Just like a regular autoencoder a VAE returns an array (image) of same dimensions as the input but variation can be introduced by tweaking the so-called latent vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd4ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from   tensorflow.keras import layers, models, losses, metrics, optimizers\n",
    "from   tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7b013",
   "metadata": {},
   "source": [
    "### Model: \"Encoder\"\n",
    "Create an encoder model with the following properties:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd20453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " flat (Flatten)                 (None, 784)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2000)         1570000     ['flat[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          512256      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " latent_mean (Dense)            (None, 2)            514         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " latent_log_var (Dense)         (None, 2)            514         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,083,284\n",
      "Trainable params: 2,083,284\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 10:11:28.907303: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# _____________________________________________________________________________\n",
    "#  Layer (type)                 Output Shape         Activation     Input      \n",
    "# =============================================================================\n",
    "#  encoder_input (InputLayer)       [(None, 28, 28, 1)]  None           \n",
    "#  encoder_flatten (Flatten)        (None, 784)          None           enc_input\n",
    "#  encoder_dense_1 (Dense)          (None, 2000)         ReLU           enc_flatten\n",
    "#  encoder_dense_2 (Dense)          (None, 256)          ReLU           enc_dense_1\n",
    "#  z_mean (Dense)                   (None, 2)            None           enc_dense_2\n",
    "#  z_log_var (Dense)                (None, 2)            None           enc_dense_2\n",
    "\n",
    "encoder_input = layers.Input(shape=(28,28,1), dtype='float32')\n",
    "encoder_flatten = layers.Flatten(name = 'flat')(encoder_input)\n",
    "encoder_dense_1 = layers.Dense(units = 2000, activation = 'relu', name = 'dense_1')(encoder_flatten)\n",
    "encoder_dense_2 = layers.Dense(units = 256, activation = 'relu', name = 'dense_2')(encoder_dense_1)\n",
    "z_mean = layers.Dense(units = 2, name = 'latent_mean')(encoder_dense_2)\n",
    "z_log_var  = layers.Dense(units = 2, name = 'latent_log_var')(encoder_dense_2)\n",
    "\n",
    "encoder = models.Model(inputs = encoder_input, outputs = (z_mean, z_log_var), name = 'encoder')\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a868c3",
   "metadata": {},
   "source": [
    "### Model: \"decoder\"\n",
    "Create a decoder model with the following properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ead7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dec_dense_1 (Dense)         (None, 256)               768       \n",
      "                                                                 \n",
      " dec_dense_2 (Dense)         (None, 2000)              514000    \n",
      "                                                                 \n",
      " dec_dense_3 (Dense)         (None, 784)               1568784   \n",
      "                                                                 \n",
      " img_out (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,083,552\n",
      "Trainable params: 2,083,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# _____________________________________________________________________________\n",
    "#  Layer (type)                Output Shape          Activation     Input\n",
    "# =============================================================================\n",
    "#  decoder_input (InputLayer)      [(None, 2)]           None\n",
    "#  decoder_dense_1 (Dense)         (None, 256)           ReLU           dec_input\n",
    "#  decoder_dense_2 (Dense)         (None, 2000)          ReLU           dec_dense_1\n",
    "#  decoder_dense_3 (Dense)         (None, 784)           Sigmoid        dec_dense_2\n",
    "#  img_out (Reshape)               (None, 28, 28, 1)     None           des_dense_3\n",
    "\n",
    "decoder_input = layers.Input(shape=(2,), dtype='float32')\n",
    "decoder_dense_1 = layers.Dense(units = 256, activation = 'relu', name = 'dec_dense_1')(decoder_input)\n",
    "decoder_dense_2 = layers.Dense(units = 2000, activation = 'relu', name = 'dec_dense_2')(decoder_dense_1)\n",
    "decoder_dense_3 = layers.Dense(units = 784, activation = 'sigmoid', name = 'dec_dense_3')(decoder_dense_2)\n",
    "img_out = layers.Reshape((28,28,1), name = 'img_out')(decoder_dense_3)\n",
    "\n",
    "decoder = models.Model(inputs = decoder_input, outputs = img_out, name = 'decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b168b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 'latent_sampling', which subclasses layers.Layer.\n",
    "# The class should perform the reparameterisation trick in its .call() method.\n",
    "\n",
    "'''---------------------------------------------------------------------------'''\n",
    "# Reparameterization Trick: z = mean + epsilon * exp(ln(variance) * 0.5)\n",
    "# epsilon = N(0,1), a unit normal with same dims as mean and variance\n",
    "'''---------------------------------------------------------------------------'''\n",
    "class latent_sampling(layers.Layer):\n",
    "    \n",
    "  def call(self, z_mean, z_log_var):\n",
    "    tf.keras.layers.Layer(trainable = True)\n",
    "    self.batch = tf.shape(z_mean)[0]\n",
    "    self.dim = tf.shape(z_mean)[1]\n",
    "    self.epsilon = tf.keras.backend.random_normal(shape=(self.batch, self.dim))\n",
    "    self.z =  z_mean + self.epsilon * tf.exp(z_log_var * 0.5)\n",
    "    \n",
    "    return self.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49561bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The 'VAE' Class. \n",
    "### The __init__ method—which will set up the layers and submodels—and the call() method.\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A Keras Model that implements a Variational Autoencoder. Model properties\n",
    "    should include the encoder and decoder models, a sampling layer, and the\n",
    "    number of latent variables in the encoded space.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        \"\"\"\n",
    "        Take in model properties and assign them to self.\n",
    "        \"\"\"\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = encoder\n",
    "        self.sampling = latent_sampling()\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Method that applies the encoder model to input data. Returns the mean\n",
    "        and ln(variance) of the encoded variables.\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encoder(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Method that applies the decoder model to a set of encoded variables.\n",
    "        Returns the generated images from the encoded data.\n",
    "        \"\"\"\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def kl_loss(self, mean, logvar):\n",
    "        \"\"\"\n",
    "        Method that calculates the Kullback-Liebler divergence between the\n",
    "        posterier distribution, N(mean, variance), and the prior, N(0,1).\n",
    "        Can be added to the model as a loss or metric, using self.add_loss and\n",
    "        self.add_metric\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the element wise KL divergence\n",
    "        kl = -0.5 * (1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "        # Sum up the divergence of all the variables in each data sample\n",
    "        kl = tf.reduce_sum(kl, axis=1)\n",
    "        # Average the divergence across all samples in the batch\n",
    "        kl = tf.reduce_mean(kl)\n",
    "        return kl\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Apply the encoder, sampling layer and decoder to the input data. Add\n",
    "        the kl divergence to the model losses and metrics. Return the generated\n",
    "        image.\n",
    "        \"\"\"\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        sampled_output = self.sampling(z_mean, z_log_var)\n",
    "        output = self.decoder(sampled_output)\n",
    "        self.add_loss(self.kl_loss(z_mean, z_log_var))\n",
    "        self.add_metric(self.kl_loss(z_mean, z_log_var), name = 'kl_loss_metric')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e608bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified loss function for the model. The standard binary cross entropy\n",
    "# takes a mean over all pixels in all images, but the VAE needs the\n",
    "# reconstruction loss to be the sum of the pixel-wise losses, averaged over\n",
    "# samples in the batch. Otherwise the reconstruction loss is becoming too small.\n",
    "\n",
    "def recon_loss(y_true, y_pred):\n",
    "    loss = tf.reduce_sum(losses.binary_crossentropy(y_true, y_pred),axis=(1, 2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a07b3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VAE model, using your encoder and decoder models. \n",
    "# Compile the model with appropriate optimizer settings, losses and metrics.\n",
    "'''\n",
    "(TASK: don't be afraid to experiment with different settings here (e.g. latent_dim))\n",
    "'''\n",
    "autoencoder = VAE(latent_dim = 2, encoder = encoder, decoder = decoder)\n",
    "\n",
    "# Default learning rate, optimizer = nAdam.\n",
    "autoencoder.compile(tf.keras.optimizers.Nadam(),loss = recon_loss,\n",
    "                    metrics = [recon_loss, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e67ed9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data set\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Function to preprocess the data \n",
    "def preprocessing(image):\n",
    "    image =  tf.expand_dims(image, -1)\n",
    "    image =  tf.image.random_flip_left_right(image)\n",
    "    image  = tf.image.convert_image_dtype(image, 'float32')\n",
    "\n",
    "    return image, image\n",
    "\n",
    "# Slice off the training data\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "\n",
    "# Preparing the data for training \n",
    "final_dataset = dataset.shuffle(1000).batch(64, drop_remainder=True).map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6fe5c",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Train the model on the images from the training set until the losses converge.\n",
    "\"history = model.fit\" allows for storing the training and validation losses in a dictionary so they can be visualized later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "504/937 [===============>..............] - ETA: 39s - loss: 186.7245 - recon_loss: 181.5594 - accuracy: 0.7914 - kl_loss_metric: 5.1651"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(final_dataset, batch_size = 256, epochs =30)\n",
    "history.history.keys()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d8734",
   "metadata": {},
   "source": [
    "### Visualize the results (plot 1)\n",
    "Create plots that show the losses and metrics, the reconstruction quality of\n",
    "the trained network, and the generative quality of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b69271",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,3, figsize = (20,5))\n",
    "\n",
    "ax[0].plot(history.history['loss'], label = 'loss')\n",
    "ax[0].plot(history.history['recon_loss'], label = 'Reconstruction loss')\n",
    "ax[0].set_ylabel('reconstruction loss')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history.history['kl_loss_metric'], label = 'KL divergence')\n",
    "ax[1].set_ylabel('KL loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(history.history['accuracy'], label = 'Accuracy')\n",
    "ax[2].set_ylabel('accuracy')\n",
    "ax[2].set_xlabel('epoch')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87df130",
   "metadata": {},
   "source": [
    "### Prediction of test data (plot 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc232238",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = autoencoder.predict(x_test/255.)\n",
    "f, ax = plt.subplots(2, 7, figsize = (15,4))\n",
    "\n",
    "# Testing the reconstruction quality of the network using the Test Images \n",
    "for i in range(7):\n",
    "    ax[0,i].imshow(x_test[i+100])\n",
    "    ax[1,i].imshow(predict[i+100,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911134a",
   "metadata": {},
   "source": [
    "### Testing the generative quality of the network (plot 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-1, 1, 10)\n",
    "y_axis = np.linspace(-1, 1, 10)\n",
    "figure = np.zeros((28 * 10, 28 * 10))\n",
    "\n",
    "# loop through each number for decoding\n",
    "for i_x, x in enumerate(x_axis):\n",
    "    for i_y, y in enumerate(y_axis):\n",
    "        latent = np.array([[x, y]])\n",
    "        #print(latent)\n",
    "        generated_image = decoder.predict(latent)[0] # decode the numbers\n",
    "        figure[i_x*28:(i_x+1)*28, i_y*28:(i_y+1)*28,] = generated_image[:,:,-1]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(figure, extent=[1,-1,1,-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf09533",
   "metadata": {},
   "source": [
    "### Task: Visualize the latent space (if latent_dim > 2 then by using T_SNE) (plot 4)\n",
    "#### Describe the latent space with respect to its structure. \n",
    "\n",
    "Remember: t-SNE is stochastic and therefore the results may appear slightly different every time it is re-run. So don't worry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code section here !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
