{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96dbe33d",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0586466c",
   "metadata": {},
   "source": [
    "# 3rd exercise: <font color=\"#C70039\">Do DBScan clustering for anomaly detection</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: Mathis Loevenich\n",
    "* Date:   02.11.2022\n",
    "* Matr. No.: 11157363 \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/400px-DBSCAN-Illustration.svg.png\" style=\"float: center;\" width=\"450\">\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too\n",
    "* This applies to all exercises throughout this course.  \n",
    "\n",
    "---------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "This notebook allows you for using the DBScan clustering algorithm for anomaly detection.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time. \n",
    "4. take the three data sets from exercize 1 and cluster them\n",
    "5. read the following <a href=\"https://stats.stackexchange.com/questions/88872/a-routine-to-choose-eps-and-minpts-for-dbscan\">article</a> for getting help estimating eps and minPts\n",
    "    * https://stats.stackexchange.com/questions/88872/a-routine-to-choose-eps-and-minpts-for-dbscan\n",
    "6. describe your findings and interpret the results\n",
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e510990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import randn\n",
    "np.random.seed(1)\n",
    "random_data = np.random.randn(50000,2)  * 20 + 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8d1be",
   "metadata": {},
   "source": [
    "The output of the below code is 94. This is the total number of noisy points. SKLearn labels the noisy points as (-1). The downside with this method is that the higher the dimension, the less accurate it becomes. You also need to make a few assumptions like estimating the right value for eps which can be challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106c724a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "minPts = 2\n",
    "eps = 3\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples = minPts, eps = eps)\n",
    "clusters = outlier_detection.fit_predict(random_data)\n",
    "list(clusters).count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421645e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_and_weights = (\n",
    "    pd.read_csv(\"data/SOCR-HeightWeight.csv\")\n",
    ").rename(\n",
    "    columns={\n",
    "        'Height(Inches)': 'height', \n",
    "        'Weight(Pounds)': 'weight'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f60432",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40cd79c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from kneed import KneeLocator # installed with pip into anaconda env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06aeae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_eps(data, minPts=2):\n",
    "    nbrs = NearestNeighbors(n_neighbors=minPts).fit(data)\n",
    "    n_samples = minPts\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    distance_desc = sorted(distances[:,n_samples-1], reverse=True)\n",
    "    # plt.plot(distance_desc)\n",
    "    kneedle = KneeLocator(range(1,len(distance_desc)+1),  #x values\n",
    "                      distance_desc, # y values\n",
    "                      S=1.0, #parameter suggested from paper\n",
    "                      curve=\"convex\", #parameter from figure\n",
    "                      direction=\"decreasing\") #parameter from figure\n",
    "    # kneedle.plot_knee_normalized()\n",
    "    return(kneedle.knee_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d155310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006339999999994461"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_eps(np.array(heights_and_weights.height).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91aefccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outliers for Human Height\n",
    "\n",
    "# hyperparameters\n",
    "data = np.array(heights_and_weights.height).reshape(-1,1)\n",
    "minPts = 2\n",
    "eps = calc_eps(data, minPts)\n",
    "\n",
    "# based on the link above: instead of DBSCAN use OPTICS\n",
    "outlier_detection = DBSCAN(min_samples = minPts, eps= eps)\n",
    "clusters = outlier_detection.fit_predict(data)\n",
    "\n",
    "list(clusters).count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90c781a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outliers for Human weight\n",
    "\n",
    "# hyperparameters\n",
    "data = np.array(heights_and_weights.weight).reshape(-1, 1)\n",
    "minPts = 2\n",
    "eps = calc_eps(data, minPts)\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples = minPts)\n",
    "clusters = outlier_detection.fit_predict(data)\n",
    "\n",
    "\n",
    "list(clusters).count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ecfe668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outliers for Penguin flipper length\n",
    "\n",
    "# hyperparameters\n",
    "data = np.array(penguins.flipper_length_mm.dropna()).reshape(-1, 1) # dropna first\n",
    "minPts = 2\n",
    "eps = calc_eps(data, minPts)\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples = minPts)\n",
    "clusters = outlier_detection.fit_predict(data)\n",
    "\n",
    "list(clusters).count(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0134a2",
   "metadata": {},
   "source": [
    "Strange is, that even with choosing the **minPts** and **eps** according to the refence above DBSCAN finds the most outliers for Human Height, only 4 outliers for Human Weight and only 7 outliers for Flipper length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b075dc5",
   "metadata": {},
   "source": [
    "It would be assumable that there should be more outliers also for human weight, as the boxplot from Ex2. suggested so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d5de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
